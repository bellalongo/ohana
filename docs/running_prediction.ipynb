{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc690b51",
   "metadata": {},
   "source": [
    "### Running Anomaly Detection with a Trained `ohana` Model\n",
    "\n",
    "This notebook demonstrates how to load a pre-trained 3D U-Net model and use it to find anomalies in a sample H2RG exposure file. We will follow the core logic found in the `ohana.predict.predictor` module.\n",
    "\n",
    "We will perform the following steps:\n",
    "1.  **Set up paths** to the model, configuration, and data.\n",
    "2.  **Load the `UNet3D` model** and its trained weights.\n",
    "3.  **Load and preprocess** the exposure data.\n",
    "4.  **Run patch-based inference** to generate a full prediction mask.\n",
    "5.  **Extract anomaly locations** from the mask.\n",
    "6.  **Visualize the results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9289a34e",
   "metadata": {},
   "source": [
    "##### Step 1: Imports and Configuration\n",
    "\n",
    "First, let's import the necessary modules from the `ohana` package and other libraries. We also define the paths to our trained model, the configuration file, and the exposure data we want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8aa37764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import label, center_of_mass\n",
    "import matplotlib.pyplot as plt\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d1585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting PyTorch to use 16 threads.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Setting PyTorch to use {os.cpu_count() or 8} threads.\")\n",
    "torch.set_num_threads(os.cpu_count() or 8)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(os.cpu_count() or 8)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(os.cpu_count() or 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "32303476",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c01e417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary classes from your ohana package\n",
    "# !NOTE: Make sure the 'ohana' directory is in your Python path\n",
    "\n",
    "from ohana.models.unet_3d import UNet3D\n",
    "from ohana.preprocessing.data_loader import DataLoader\n",
    "from ohana.preprocessing.preprocessor import Preprocessor\n",
    "from ohana.visualization.plotter import ResultVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0dca8073",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Configuration \"\"\"\n",
    "# !NOTE: Replace these with the actual paths to your files.\n",
    "\n",
    "# Path to the trained model\n",
    "MODEL_PATH = \"../trained_models/old_best_model_unet3d.pth\"\n",
    "\n",
    "# Path to the config file that was used for model training\n",
    "CONFIG_PATH = \"../configs/creator_config.yaml\"\n",
    "\n",
    "# Path to the exposure you want to run the predictions on\n",
    "EXPOSURE_PATH = \"/Volumes/jwst/ilongo/raw_data/18220_Euclid_SCA/ap30_100k_0p8m0p3_fullnoi_E001_18220.fits\"\n",
    "\n",
    "# Directory where model predictions will be stored\n",
    "OUTPUT_DIR = \"prediction_outputs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6fa2b32f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9115c7",
   "metadata": {},
   "source": [
    "##### Step 2: Load the Trained Model\n",
    "\n",
    "Next, we load the `UNet3D` model architecture and the saved weights from your `.pth` file. The code includes a step to handle models that were trained using `nn.DataParallel` on multiple GPUs. This logic comes directly from the `Predictor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73fef070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: ../trained_models/old_best_model_unet3d.pth\n",
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the configuration file\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "\n",
    "# Initialize the model\n",
    "# The number of classes should match the 'num_classes' in your config\n",
    "model = UNet3D(n_channels=1, n_classes=config['num_classes'])\n",
    "\n",
    "# Load the trained weights\n",
    "print(f\"Loading model from: {MODEL_PATH}\")\n",
    "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "\n",
    "# Handle models saved with nn.DataParallel\n",
    "if next(iter(state_dict)).startswith('module.'):\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        name = k[7:]  # remove `module.`\n",
    "        new_state_dict[name] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "else:\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "model.to(device)\n",
    "model.eval() # Set the model to evaluation mode\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f6aec8",
   "metadata": {},
   "source": [
    "##### Step 3: Load and Preprocess the Exposure Data\n",
    "\n",
    "We'll use the `DataLoader` to load the raw exposure cube and the `Preprocessor` to clean it and create the difference-image cube, which is the actual input to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0205b8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ReferencePixelCorrector initialized with x_opt=64, y_opt=4.\n",
      "Preprocessor initialized. Reference pixel correction: Enabled.\n",
      "Loading and preprocessing exposure: /Volumes/jwst/ilongo/raw_data/18220_Euclid_SCA/ap30_100k_0p8m0p3_fullnoi_E001_18220.fits\n",
      "Loading data from Multi-Extension FITS file: /Volumes/jwst/ilongo/raw_data/18220_Euclid_SCA/ap30_100k_0p8m0p3_fullnoi_E001_18220.fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading FITS extensions:  65%|██████▍   | 292/450 [05:15<02:43,  1.03s/it]"
     ]
    }
   ],
   "source": [
    "# Initialize data loader and preprocessor\n",
    "data_loader = DataLoader()\n",
    "preprocessor = Preprocessor()\n",
    "\n",
    "# Load and process the exposure\n",
    "print(f\"Loading and preprocessing exposure: {EXPOSURE_PATH}\")\n",
    "raw_exposure = data_loader.load_exposure(EXPOSURE_PATH)\n",
    "processed_cube = preprocessor.process_exposure(raw_exposure) # Shape: (T, H, W)\n",
    "\n",
    "print(f\"Processed data cube shape: {processed_cube.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbcceb9",
   "metadata": {},
   "source": [
    "##### Step 4: Run Patch-Based Inference\n",
    "\n",
    "The model works on small, overlapping patches of the data cube. We will now extract these patches, run the model on each one, and stitch the results back together into a single, full-sized prediction mask. This process is identical to the `predict` method in the `Predictor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b42a1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running inference on all patches...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/81 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     31\u001b[39m normalized_tensor = normalized_tensor.view(b, c_in, t, h, w) \u001b[38;5;66;03m# .view() is fine here as normalized_tensor is new and contiguous\u001b[39;00m\n\u001b[32m     33\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m     logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnormalized_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     central_logits = logits[:, :, logits.shape[\u001b[32m2\u001b[39m] // \u001b[32m2\u001b[39m, :, :]\n\u001b[32m     36\u001b[39m     pred_patch_mask = torch.argmax(central_logits, dim=\u001b[32m1\u001b[39m).squeeze(\u001b[32m0\u001b[39m).cpu().numpy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Excalibur/ohana/jpl_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Excalibur/ohana/jpl_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Excalibur/ohana/docs/../ohana/models/unet_3d.py:267\u001b[39m, in \u001b[36mUNet3D.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    265\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Encoder: compute and store skip features\"\"\"\u001b[39;00m\n\u001b[32m    266\u001b[39m \u001b[38;5;66;03m# Skip 1: (N, 32,, T, H, W)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m267\u001b[39m x1 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# Skip 2: (N, 64, T/2, H/2, W/2)\u001b[39;00m\n\u001b[32m    270\u001b[39m x2 = \u001b[38;5;28mself\u001b[39m.down1(x1)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Excalibur/ohana/jpl_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Excalibur/ohana/jpl_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Excalibur/ohana/docs/../ohana/models/unet_3d.py:47\u001b[39m, in \u001b[36mDoubleConv3D.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m     39\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m        Run the 2 Conv3D-BatchNorm3D-ReLU blocks, perseving th 3x3x3 kernel, only changing\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[33;03m        the channel dimenstion\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     45\u001b[39m \u001b[33;03m            torch.Tensor: output of shape (N, C_out, T, H, W)\u001b[39;00m\n\u001b[32m     46\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdouble_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Excalibur/ohana/jpl_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Excalibur/ohana/jpl_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Excalibur/ohana/jpl_env/lib/python3.11/site-packages/torch/nn/modules/container.py:217\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[32m    216\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m217\u001b[39m         \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    218\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Excalibur/ohana/jpl_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1510\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1511\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Excalibur/ohana/jpl_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1515\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1516\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1517\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1518\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1519\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1520\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1522\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1523\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Excalibur/ohana/jpl_env/lib/python3.11/site-packages/torch/nn/modules/conv.py:610\u001b[39m, in \u001b[36mConv3d.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    609\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m610\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/Excalibur/ohana/jpl_env/lib/python3.11/site-packages/torch/nn/modules/conv.py:605\u001b[39m, in \u001b[36mConv3d._conv_forward\u001b[39m\u001b[34m(self, input, weight, bias)\u001b[39m\n\u001b[32m    593\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.padding_mode != \u001b[33m\"\u001b[39m\u001b[33mzeros\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    594\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.conv3d(\n\u001b[32m    595\u001b[39m         F.pad(\n\u001b[32m    596\u001b[39m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m._reversed_padding_repeated_twice, mode=\u001b[38;5;28mself\u001b[39m.padding_mode\n\u001b[32m   (...)\u001b[39m\u001b[32m    603\u001b[39m         \u001b[38;5;28mself\u001b[39m.groups,\n\u001b[32m    604\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m605\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    606\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgroups\u001b[49m\n\u001b[32m    607\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "# Set device to 'cpu'\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "# --- Patch Extraction Logic ---\n",
    "print(\"Extracting all patches...\")\n",
    "patch_size = tuple(config['patch_size'])\n",
    "overlap = config['overlap']\n",
    "step_h = patch_size[0] - overlap\n",
    "step_w = patch_size[1] - overlap\n",
    "_, H, W = processed_cube.shape\n",
    "prediction_mask = np.zeros((H, W), dtype=np.uint8)\n",
    "\n",
    "patches_with_coords = []\n",
    "for r in range(0, H - patch_size[0] + 1, step_h):\n",
    "    for c in range(0, W - patch_size[1] + 1, step_w):\n",
    "        patch_data = processed_cube[:, r:r+patch_size[0], c:c+patch_size[1]]\n",
    "        patches_with_coords.append((patch_data, (r, c)))\n",
    "print(f\"Extracted {len(patches_with_coords)} patches.\")\n",
    "\n",
    "\n",
    "# --- Run BATCHED Inference on CPU ---\n",
    "print(f\"Running CPU inference with a batch size of {BATCH_SIZE}...\")\n",
    "for i in tqdm(range(0, len(patches_with_coords), BATCH_SIZE)):\n",
    "    # Get a \"batch\" of patches from the list\n",
    "    batch_patches_with_coords = patches_with_coords[i:i+BATCH_SIZE]\n",
    "    batch_patch_data = [item[0] for item in batch_patches_with_coords]\n",
    "    \n",
    "    # Stack patches into a single batch tensor\n",
    "    input_tensor = torch.from_numpy(np.stack(batch_patch_data)).float().unsqueeze(1).to(device)\n",
    "\n",
    "    # --- Normalize the ENTIRE batch at once ---\n",
    "    b, c_in, t, h, w = input_tensor.shape\n",
    "    tensor_flat = input_tensor.reshape(b, -1)\n",
    "    min_val = tensor_flat.min(dim=1, keepdim=True)[0]\n",
    "    max_val = tensor_flat.max(dim=1, keepdim=True)[0]\n",
    "    normalized_tensor = (tensor_flat - min_val) / (max_val - min_val + 1e-6)\n",
    "    normalized_tensor = normalized_tensor.view(b, c_in, t, h, w)\n",
    "\n",
    "    # --- Run the model on the entire batch ---\n",
    "    with torch.no_grad():\n",
    "        logits = model(normalized_tensor)\n",
    "        central_logits = logits[:, :, logits.shape[2] // 2, :, :]\n",
    "        pred_batch_mask = torch.argmax(central_logits, dim=1).cpu().numpy()\n",
    "    \n",
    "    # --- Place the predicted patches into the full mask ---\n",
    "    for j, (r, c) in enumerate([item[1] for item in batch_patches_with_coords]):\n",
    "        pred_patch = pred_batch_mask[j]\n",
    "        ph, pw = pred_patch.shape\n",
    "        current_mask_region = prediction_mask[r:r+ph, c:c+pw]\n",
    "        prediction_mask[r:r+ph, c:c+pw] = np.maximum(current_mask_region, pred_patch)\n",
    "\n",
    "# --- SAVE ALL OUTPUTS ---\n",
    "processed_data_path = os.path.join(OUTPUT_DIR, 'processed_cube.npy')\n",
    "mask_path = os.path.join(OUTPUT_DIR, 'prediction_mask.npy')\n",
    "\n",
    "np.save(processed_data_path, processed_cube)\n",
    "print(f\"✅ Processed data cube saved to: {processed_data_path}\")\n",
    "\n",
    "np.save(mask_path, prediction_mask)\n",
    "print(f\"✅ Full prediction mask saved to: {mask_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
