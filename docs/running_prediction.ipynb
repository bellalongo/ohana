{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc690b51",
   "metadata": {},
   "source": [
    "### Running Anomaly Detection with a Trained `ohana` Model\n",
    "\n",
    "This notebook demonstrates how to load a pre-trained 3D U-Net model and use it to find anomalies in a sample H2RG exposure file. We will follow the core logic found in the `ohana.predict.predictor` module.\n",
    "\n",
    "We will perform the following steps:\n",
    "1.  **Set up paths** to the model, configuration, and data.\n",
    "2.  **Load the `UNet3D` model** and its trained weights.\n",
    "3.  **Load and preprocess** the exposure data.\n",
    "4.  **Run patch-based inference** to generate a full prediction mask.\n",
    "5.  **Extract anomaly locations** from the mask.\n",
    "6.  **Visualize the results.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9289a34e",
   "metadata": {},
   "source": [
    "##### Step 1: Imports and Configuration\n",
    "\n",
    "First, let's import the necessary modules from the `ohana` package and other libraries. We also define the paths to our trained model, the configuration file, and the exposure data we want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8aa37764",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import yaml\n",
    "import numpy as np\n",
    "import os\n",
    "from collections import OrderedDict\n",
    "from tqdm import tqdm\n",
    "from scipy.ndimage import label, center_of_mass\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7613e09d",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "src_path = os.path.join(project_root, 'src')\n",
    "\n",
    "if src_path not in sys.path:\n",
    "    sys.path.insert(0, src_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213d1585",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Setting PyTorch to use {os.cpu_count() or 8} threads.\")\n",
    "torch.set_num_threads(os.cpu_count() or 8)\n",
    "os.environ[\"OMP_NUM_THREADS\"] = str(os.cpu_count() or 8)\n",
    "os.environ[\"MKL_NUM_THREADS\"] = str(os.cpu_count() or 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32303476",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.insert(0, '../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01e417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary classes from your ohana package\n",
    "# !NOTE: Make sure the 'ohana' directory is in your Python path\n",
    "\n",
    "from ohana.models.unet_3d import UNet3D\n",
    "from ohana.predict.predictor import Predictor\n",
    "from ohana.preprocessing.data_loader import DataLoader\n",
    "from ohana.preprocessing.preprocessor import Preprocessor\n",
    "from ohana.visualization.plotter import ResultVisualizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dca8073",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Configuration \"\"\"\n",
    "# !NOTE: Replace these with the actual paths to your files.\n",
    "\n",
    "# Path to the trained model\n",
    "MODEL_PATH = \"../trained_models/old_best_model_unet3d.pth\"\n",
    "\n",
    "# Path to the config file that was used for model training\n",
    "CONFIG_PATH = \"../configs/creator_config.yaml\"\n",
    "\n",
    "# Path to the exposure you want to run the predictions on\n",
    "EXPOSURE_PATH = \"/Volumes/jwst/ilongo/raw_data/18220_Euclid_SCA/ap30_100k_0p8m0p3_fullnoi_E001_18220.fits\"\n",
    "\n",
    "# Path to where the processed exposure is saved to (MUST BE .NPY)\n",
    "PROCESSED_EXPOSURE_FILE = 'processed_ap30_100k_0p8m0p3_fullnoi_E001_18220.npy'\n",
    "\n",
    "# Directory where model predictions will be stored\n",
    "OUTPUT_DIR = \"prediction_outputs\"\n",
    "\n",
    "# Where processed exposure will be saved\n",
    "PROCESSED_DATA_PATH = os.path.join(OUTPUT_DIR, PROCESSED_EXPOSURE_FILE)\n",
    "\n",
    "# Where prediction mask with be saved\n",
    "MASK_PATH = os.path.join(OUTPUT_DIR, 'prediction_mask.npy')\n",
    "\n",
    "# Where detections will be saved\n",
    "DETECTIONS_PATH = os.path.join(OUTPUT_DIR, 'detections.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa2b32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d9115c7",
   "metadata": {},
   "source": [
    "##### Step 2: Load the Trained Model\n",
    "\n",
    "Next, we load the `UNet3D` model architecture and the saved weights from your `.pth` file. The code includes a step to handle models that were trained using `nn.DataParallel` on multiple GPUs. This logic comes directly from the `Predictor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fef070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load configuration\n",
    "with open(CONFIG_PATH, 'r') as f:\n",
    "    config = yaml.safe_load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7c09bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model architecture\n",
    "model = UNet3D(n_channels=1, n_classes=config['num_classes'])\n",
    "\n",
    "# Load trained weights, handling DataParallel prefixes if they exist\n",
    "print(f\"Loading model from: {MODEL_PATH}\")\n",
    "state_dict = torch.load(MODEL_PATH, map_location=device)\n",
    "\n",
    "if next(iter(state_dict)).startswith('module.'):\n",
    "    new_state_dict = OrderedDict()\n",
    "    for k, v in state_dict.items():\n",
    "        new_state_dict[k[7:]] = v\n",
    "    model.load_state_dict(new_state_dict)\n",
    "else:\n",
    "    model.load_state_dict(state_dict)\n",
    "\n",
    "model.to(device)\n",
    "model.eval() # Set model to evaluation mode\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f6aec8",
   "metadata": {},
   "source": [
    "##### Step 3: Load and Preprocess the Exposure Data\n",
    "\n",
    "We'll use the `DataLoader` to load the raw exposure cube and the `Preprocessor` to clean it and create the difference-image cube, which is the actual input to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0205b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our helper classes\n",
    "data_loader = DataLoader()\n",
    "preprocessor = Preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee820007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the raw data\n",
    "raw_exposure = data_loader.load_exposure(EXPOSURE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da44b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the data. This will save the result to PROCESSED_DATA_PATH.\n",
    "# If the file already exists, it will load it from the cache.\n",
    "processed_cube = preprocessor.process_exposure(\n",
    "    raw_exposure_cube=raw_exposure,\n",
    "    save_path=PROCESSED_DATA_PATH   \n",
    ")\n",
    "print(f\"Preprocessing complete. Cube shape: {processed_cube.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cbcceb9",
   "metadata": {},
   "source": [
    "##### Step 4: Run Patch-Based Inference\n",
    "\n",
    "The model works on small, overlapping patches of the data cube. We will now extract these patches, run the model on each one, and stitch the results back together into a single, full-sized prediction mask. This process is identical to the `predict` method in the `Predictor` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8dd7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2095c5c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Patch Extraction ---\n",
    "print(\"Extracting all patches...\")\n",
    "patch_size = tuple(config['patch_size'])\n",
    "overlap = config['overlap']\n",
    "step_h, step_w = patch_size[0] - overlap, patch_size[1] - overlap\n",
    "_, H, W = processed_cube.shape\n",
    "prediction_mask = np.zeros((H, W), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2505f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_with_coords = []\n",
    "for r in range(0, H - patch_size[0] + 1, step_h):\n",
    "    for c in range(0, W - patch_size[1] + 1, step_w):\n",
    "        patches_with_coords.append((processed_cube[:, r:r+patch_size[0], c:c+patch_size[1]], (r, c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953b274a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Running inference with batch size {BATCH_SIZE}...\")\n",
    "for i in tqdm(range(0, len(patches_with_coords), BATCH_SIZE)):\n",
    "    batch_patches_with_coords = patches_with_coords[i:i+BATCH_SIZE]\n",
    "    batch_patch_data = [item[0] for item in batch_patches_with_coords]\n",
    "    \n",
    "    input_tensor = torch.from_numpy(np.stack(batch_patch_data)).float().unsqueeze(1).to(device)\n",
    "\n",
    "    # Normalize batch\n",
    "    b, c_in, t, h, w = input_tensor.shape\n",
    "    tensor_flat = input_tensor.reshape(b, -1)\n",
    "    min_val, max_val = tensor_flat.min(dim=1, keepdim=True)[0], tensor_flat.max(dim=1, keepdim=True)[0]\n",
    "    normalized_tensor = (tensor_flat - min_val) / (max_val - min_val + 1e-6)\n",
    "    normalized_tensor = normalized_tensor.view(b, c_in, t, h, w)\n",
    "\n",
    "    # Predict\n",
    "    with torch.no_grad():\n",
    "        logits = model(normalized_tensor)\n",
    "        central_logits = logits[:, :, logits.shape[2] // 2, :, :]\n",
    "        pred_batch_mask = torch.argmax(central_logits, dim=1).cpu().numpy()\n",
    "    \n",
    "    # Stitch results into the main mask\n",
    "    for j, (r, c) in enumerate([item[1] for item in batch_patches_with_coords]):\n",
    "        pred_patch = pred_batch_mask[j]\n",
    "        ph, pw = pred_patch.shape\n",
    "        prediction_mask[r:r+ph, c:c+pw] = np.maximum(prediction_mask[r:r+ph, c:c+pw], pred_patch)\n",
    "\n",
    "# --- Save the final mask ---\n",
    "np.save(MASK_PATH, prediction_mask)\n",
    "print(f\"Inference complete. Prediction mask saved to: {MASK_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b0470a",
   "metadata": {},
   "source": [
    "##### Step 5: Extract Anomaly Locations\n",
    "\n",
    "Now that we have the full 2D prediction mask, we use a connected-components algorithm to find the center of each detected anomaly region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbee3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = []\n",
    "class_map_inv = {v: k for k, v in config.get('class_map', {}).items()}\n",
    "\n",
    "for class_idx, class_name in class_map_inv.items():\n",
    "    if class_idx == 0: continue\n",
    "    class_mask = (prediction_mask == class_idx).astype(int)\n",
    "    labeled_array, num_features = label(class_mask)\n",
    "    if num_features > 0:\n",
    "        centers = center_of_mass(class_mask, labeled_array, range(1, num_features + 1))\n",
    "        for center in centers:\n",
    "            detections.append({'type': class_name, 'location_px': [int(round(c)) for c in center]})\n",
    "\n",
    "print(f\"Found {len(detections)} objects. Saving detections list...\")\n",
    "with open(DETECTIONS_PATH, 'w') as f:\n",
    "    json.dump(detections, f, indent=4)\n",
    "print(f\"Detections list saved to: {DETECTIONS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e36bf78",
   "metadata": {},
   "source": [
    "##### Step 6: Visualize the Results\n",
    "\n",
    "Finally, we use the `ResultVisualizer` to overlay our outputs on the processed data cube. Since all files were saved in the previous steps, this cell simply loads them for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc0a2cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating visualization...\")\n",
    "visualizer = ResultVisualizer(\n",
    "    processed_data_path=PROCESSED_DATA_PATH,\n",
    "    prediction_mask_path=MASK_PATH\n",
    ")\n",
    "visualizer.load_detection_list(results_path=DETECTIONS_PATH)\n",
    "visualizer.plot_full_mask_overlay()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jpl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
